---
title: "Machine Learning with a Heart"
subtitle: "Predicting Heart Disease"
author: |
  | [Ogundepo Ezekiel Adebayo](https://bit.ly/gbganalyst)
  | [I'm on Twitter](https://twitter.com/gbganalyst)

date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    theme: united
    highlight: espresso
    toc: true
    number_sections: true
    toc_depth: 3
    toc_float: true
    code_download: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Heart disease refers to several types of heart conditions and it is the [number one cause of death worldwide](www.world-heart-federation.org/resources/cardiovascular-diseases-cvds-global-facts-figures/). To prevent heart disease, we must first learn how to reliably detect it. The heart disease data used in this study has various measurements on patients health and cardiovascular statistics.

![Heart disease](Images/Heart_disease.PNG)

Source: UChicago Medicine

# Source of data

This study used dataset from a study of heart disease that has been open to the public at the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php) which is being maintained by the Center for Machine Learning and Intelligent Systems at the University of California, Irvine.


## Import packages

We shall use different set of packages in R

* Data preparation and exploration
* Machine learning packages
* Table formating

```{r R_library, message=FALSE, warning=FALSE, comment=FALSE, include=TRUE}

#Import packages

data_exploration_packages <- c("tidyverse", "plotly", "openxlsx")

machine_learning_packages <- c("caret","MASS", "car", "kernlab","rpart","randomForest","class","ada", "rda","e1071", "nnet","ipred", "dbarts", "klaR", "glmnet", 'earth')

table_formating_packages <- c("knitr","kableExtra")


if(!require(install.load)){
  install.packages("install.load")
}

install.load::install_load(c(data_exploration_packages, machine_learning_packages, table_formating_packages))

```


## Load and prepare the dataset

As a first step we must load the dataset. 

```{r Training dataset, message=FALSE, warning=FALSE, comment=FALSE, cache=TRUE,include=TRUE}

# Import data

data_values <- read_csv("Data from DRIVENDATA/train_values.csv")
data_labels <- read_csv("Data from DRIVENDATA/train_labels.csv")

# Concatenating the two datasets 

disease_data <- bind_cols(data_values, data_labels[,2])

```


> **Overview of heart disease dataset**

```{r kable1, eval=T, include=T,echo=F}

kable(disease_data, caption = 'Heart disease data', align = rep('c', 15)) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), fixed_thead = T) %>% scroll_box(width = "900px", height = "500px")
 
```


## Description of the dataset

There are 14 columns in the dataset, where the patient_id column is a unique and random identifier. The remaining 13 features are described below.

* slope_of_peak_exercise_st_segment (type: int): the slope of the peak exercise ST segment, an electrocardiography read out indicating quality of blood flow to the heart

* thal (type: categorical): results of thallium stress test measuring blood flow to the heart, with possible values normal, fixed_defect, reversible_defect

* resting_blood_pressure (type: int): resting blood pressure

* chest_pain_type (type: int): chest pain type (4 values)

* num_major_vessels (type: int): number of major vessels (0-3) colored by flourosopy

* fasting_blood_sugar_gt_120_mg_per_dl (type: binary): fasting blood sugar > 120 mg/dl

* resting_ekg_results (type: int): resting electrocardiographic results (values 0,1,2)

* serum_cholesterol_mg_per_dl (type: int): serum cholestoral in mg/dl

* oldpeak_eq_st_depression (type: float): oldpeak = ST depression induced by exercise relative to rest, a measure of abnormality in electrocardiograms

* sex (type: binary): 0: female, 1: male

* age (type: int): age in years

* max_heart_rate_achieved (type: int): maximum heart rate achieved (beats per minute)

* exercise_induced_angina (type: binary): exercise-induced chest pain (0: False, 1: True)


# Data preparation and preprocessing

## Transform column data type

There are eight columns in this dataset which do not have the correct data type as expected. This is a common situation, as the methods used to automatically determine data type when loading files can fail sometimes. The code in the cell below converted the variables into the right format i.e. factors. 


```{r heart disease dataset, message=FALSE, warning=TRUE, comment=FALSE, cache=TRUE,include=TRUE}

disease_data <-  disease_data[-1] # patient_id column dropped

factor_variable_position <- c(1, 2, 4, 6, 7, 10, 13, 14)

disease_data <- disease_data %>% mutate_at(vars(factor_variable_position),as_factor)

```


The categorical features are now coded well. Additionally, the label is now coded as a binary variable. 

## Examine class label 

As shown in the chart, we have 100 (56%) patients that have no heart disease while 80 (44%) of the patients have heart disease. 

```{r chart 1}

theme_set(theme_bw())# The current theme is automatically applied to every plot we draw

disease_data <- disease_data %>% mutate(heart_disease_present= fct_recode(heart_disease_present, absent = "0", present = "1"))

chart <- disease_data %>% count(heart_disease_present) %>% mutate(pct= round(n/sum(n)*100)) %>% ggplot(aes(x = heart_disease_present, y=n, fill= heart_disease_present)) + geom_bar(stat = 'identity', width = 0.4, show.legend = FALSE)+ labs(x='Heart disease present', y='Number of patients', caption = "Source: Heart disease data") +
  scale_fill_manual(values = c( "present" = "red", "absent"= "green"), aesthetics = 'fill') +
  geom_text(aes(label= str_c(pct, "%")), vjust= 4.5, size=2.5, colour='black') +
theme(legend.position = 'top', axis.title.y = element_text(size = 12, face='bold'), axis.title.x =element_text(size = 12, face='bold'), axis.text.x = element_text(angle = 50, vjust = 0.3, face = 'bold'))

ggplotly(chart, tooltip = c("x", "y"))

```


## Visualize class separation by numeric features

The primary goal of visualization for classification problems is to understand which features are useful for class separation. In this section, we will start by visualizing the separation quality of numeric features. 

```{r chart2}

# If you have column name as a character vector (e.g. col= 'age'), use .data[[col]]. If the column name or expression is supplied by the user, you can pass it to aes() or vars() using {{col}} i.e. curly curly syntax.(This is rlang syntax for writing a function)

plot_box = function(df, cols, col_x='heart_disease_present'){
  for(col in cols){
    p = ggplot(df, aes(x = .data[[col_x]], y = .data[[col]], fill = .data[[col_x]]))+ geom_boxplot(show.legend = FALSE) +
      scale_fill_manual(values = c( "present" = "red", "absent" = "green"), aesthetics = 'fill') +
      labs(x='Heart disease present', y= str_c(col),
       title=str_c('Box plot of', col, '\n vs.',
      col_x, sep = ' '), caption = "Source: Heart disease data")  + theme(axis.text.x = element_text(face = 'bold'), axis.title.y = element_text(size = 12, face='bold'), axis.title.x =element_text(size = 12, face='bold'))
    
  print(p) 
  }
}

num_cols = disease_data %>% select_if(is.numeric) %>% colnames()

plot_box(disease_data, num_cols)    
```


Box plots are useful, since by construction we are forced to focus on the overlap (or not) of the quartiles of the distribution. In this case, we might ask the question like: is there sufficient differences in the quartiles for the feature to be useful in separation the label classes? There are two cases displayed above:

1. For age, max_heart_rate_achieved,oldpeak_eq_st_depression, num_major_vessels, there is useful separation between absent and present of heart disease patients. As one might expect, older people tends to have heart disease compared to the younger one. 

2. On the other hand, resting_blood_pressure and serum_cholesterol_mg_per_dl does not seem to matter. 

## Visualizing class separation by categorical features

Now we will turn to the problem of visualizing the ability of categorical features to separate classes of the label. Ideally, a categorical feature will have very different counts of the categories for each of the label values. A good way to visualize these relationships is with bar plots. The code in the cell below creates side by side plots of the categorical variables for each of the labels categories.

```{r chart3}

# Since the facet_var will be supplied by the user, we pass curly curly syntax to vars() using {{facet_var}}

plot_bars = function(df, cat_cols, facet_var){
   for(col in cat_cols){
    p = ggplot(df, aes(x = .data[[col]], fill = .data[[col]])) + 
      geom_bar(show.legend = F)  +
      labs(x = col, y = "Number of patients", title = str_c('Bar plot of ', col, '\nfor heart disease')) +
      facet_wrap(vars({{facet_var}})) +
      theme(axis.title.y = element_text(size = 12, face = 'bold'), axis.title.x = element_text(size = 12, face = 'bold'),
            axis.text.x = element_text(angle = 90, hjust = 1, face = 'bold'))
    
    print(p)
    
  }
}

cat_cols <- disease_data %>% select_if(is.factor) %>% colnames()
cat_cols <- cat_cols[-8] # removing the class label

plot_bars(disease_data, cat_cols, heart_disease_present)    
```


There is a lot of information in these plots. The key to interpretation of these plots is comparing the proportion of the categories for each of the label values. If these proportions are distinctly different for each label category, the feature is likely to be useful in separating the label.  

There are several cases evident in these plots:

1. Some features such as slope_of_peak_exercise_st_segment, thal, and chest_pain_type have significantly different distribution of categories between the label categories.

2. Others features such as fasting_blood_sugar_gt_120_mg_per_dl, exercise_induced_angina and sex  show small differences, but these differences are unlikely to be significant.

3. Other feature like resting_ekg_results has a dominant category with very few cases of other categories. These features will likely have very little power to separate the cases.

Notice that only a few of these categorical features will be useful in separating the cases.

# Model building

In this secton we will perform **two-class classification** using **binary classifiers**. A classifier is a machine learning model that separates the **label** into categories or **classes**. In other words, classification models are **supervised** machine learning models which predict a categorical label. Common examples of classifiers include logistic regression, K Nearest Neighbour (KNN), Support Vector Machines (SVM), Random forest (RF), and Artificial neural network (NN). Some classifiers such as logistic regression, artificial neural network, gaussian process and random forest are known to predict probability of a given instance belonging to a particular class and are therefore called probabilistic classifiers. Classifiers of this nature use statistical inference to categorize the best label for a given instance. A predicted probability can then be converted into a class value by selecting the class label that has the highest probability. Unlike other algorithm such as K nearest neighbour which simply output the best class for a given instance. Classifiers such as logistic regression and adaboost were designed primarily for solving binary class problem and therefore will not work for multi-class problem.


In this case, our machine learning models used heart disease data to determine if a particular patient has heart disease or not. Thus, heart disease of patient is the classes we must predict. 

## Split the heart disease dataset into training and test datasets

we will create randomly sampled training and test data sets. The `createDataPartition()` function from the R caret package is used  to create indices for the training data sample. In this case 80% of the data will be used  for training the model. Since this data set is small, only 36 cases will be included in the test dataset. Execute this code and note the dimensions of the resulting data frame.

```{r datasplit}

# Create the training and test datasets for disease_dataset

# Step 1: Get row numbers for the training data
partition <- createDataPartition(disease_data$heart_disease_present, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
train_diseaseData <- disease_data[partition,] # Create the training sample

cat("The dimension of the training set is (",dim(train_diseaseData),")")

# Step 3: Create the test dataset
test_diseaseData <- disease_data[-partition,] # Create the test sample

cat("The dimension of test set is (", dim(test_diseaseData),")")

```

## Scale numeric features

Numeric features must be rescaled so they have a similar range of values. Rescaling prevents features from having an undue influence on model training simply because then have a larger range of numeric variables. 

The code in the cell below uses the `preProcess()` function from the caret function. The processing is as follows:

1. The preprocessing model object is computed. In this case the processing includes centering and scaling the numeric feature. Notice that this model is fit only to the training data.

2. The scaling is applied to both the test and training partitions.

```{r Scaling}

# Scaling the continuous variables

preProcess_scale_model  <- preProcess(train_diseaseData, method = c("center", "scale"))

# Here is what preProcess_scale_model does.
# It only normalize the 5 continuous variables
print(preProcess_scale_model)

train_diseaseData = predict(preProcess_scale_model, train_diseaseData)

test_diseaseData = predict(preProcess_scale_model, test_diseaseData)
```


## One hot encoding

To convert all the nominal or factor variables to numeric, we need to create a **dummy variable** for each category of the categorical variables. Only one dummy variable is coded with a one for each set of categories. This is known as **one hot encoding**. By using numeric dummy variable, the entire training feature array is now numeric. To do this, we use `dummyVars()` function from the caret package. A predict method is applied to create numeric model matrices for training and test. 

```{r One hot encoding, message=F, comment=F, warning=F}

# Removing the class column on train data to be able to create a one hot encoding

xtrain= train_diseaseData[-length(train_diseaseData)]

# `fullRank = T` to avoid dummmy trap

dummies <- dummyVars( "~.", data = xtrain, fullRank = T)

# Here is what `dummyVars()` does.
# It created `one hot encoding` to the nominal variables

print(dummies)

xtrain_dummy <-  predict(dummies, newdata = xtrain)

# Convert to dataframe

xtrain <- as_tibble(xtrain_dummy)

# Apply onehot to `test` data

# Removing the class column on test data

xtest= test_diseaseData[-length(test_diseaseData)]

xtest_dummy <- predict(dummies, newdata= xtest)

# Convert to dataframe

xtest <- as_tibble(xtest_dummy)

```

## Feature selection

**Feature selection** can be an important part of model selection. In supervised learning, including features in a model which do not provide information on the label, is useless at best, and may prevent generalization at worst.

Feature selection can involve application of several methods. Two important methods include:
  
1. Eliminating features with **low variance** and **zero variance**. Zero variance features are comprised of the same values. Low variance features arise from features with most values the same and with few unique values. One way low variance features can arise, is from dummy variables for categories with very few members. The dummy variable will be mostly 0s with very few 1s. 

2. Training machine learning models with features that are **uninformative** can create a variety of problems. An uninformative feature does not significantly improve model performance. In many cases, the noise in the uninformative features will increase the variance of the model predictions. In other words, uninformative models are likely to reduce the ability of the machine learning model to generalize.   

```{r Feature selection, comment=F, message=F}

# Eliminate low variance features

near_zero = nearZeroVar(xtrain, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)

low_variance_cols <- near_zero[(near_zero$zeroVar == TRUE) | (near_zero$nzv == TRUE), ]

print(low_variance_cols) 

```

We will remove the features that will not make our models to generalize well on the test and validation datasets.


```{r low_variance_cols}

# Remove low variance columns on train set

xtrain <- xtrain %>% dplyr::select(-c(resting_ekg_results.1, thal.fixed_defect))

# Appending Y to the `xtrainDummy` dataset

xytrain <- bind_cols(xtrain, y=train_diseaseData$heart_disease_present)

# Remove low variance columns on test set

xtest <- xtest %>% dplyr::select(-c(resting_ekg_results.1, thal.fixed_defect))

# Appending Y to the `xtestDummy` dataset

xytest <- bind_cols(xtest, y=test_diseaseData$heart_disease_present)

```



```{r variable stored}

# Store X and Y for later use.
xtrain <- xtrain 
ytrain <- xytrain$y

xtest <- xtest
ytest <- xytest$y

ntr <- nrow(xytrain)
nte <- nrow(xytest)

```


## Evaluation metric

The metric used for the evaluation of the performance of each model is logarithmic loss. In order to calculate Log Loss, the classifier must assign a probability to each class rather than simply yielding the most likely class. Mathematically Log Loss is defined as :

$$\text{Logloss}= - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i)\right]$$

where:

$n$ - number of observations

$log$ - the natural logarithm

$y$ - a binary indicator ($0$ or $1$) of whether class label $c$ is the correct classification for observation $o$

$p$ - the model's predicted probability that observation $o$ is of class $c$.

Logarithmic loss provides a steep penalty for predictions that are both confident and wrong. That is, it takes into account the uncertainty of our model prediction based on how much it varies from the actual label. Logloss has no upper bound and it exists on the range $[0, \infty)$. Logloss nearer to $0$ indicates higher accuracy, whereas if the logloss is away from $0$ then it indicates lower accuracy. In general, the least logloss gives greater accuracy for the classifier. The goal is to minimize the logloss and a perfect classifier would have a logloss of precisely zero while less ideal classifiers have progressively larger values of logloss.


## Computational section

The predictive performance of machine learning models depend on the structure of the dataset and proper data preparation will ensure the models work optimally. Since the best machine learning method on dataset cannot be known beforehand, in this section, we consider different catalogs of **machine learning** algorithms which included **parametric** and **non parametric** on **heart disease** data and we evaluated the performance of each model with **logloss** metric on **test** data.

### R functions

We wrote two functions such as `prob.prediction()` and `logloss()`.

`prob.prediction()`: This function takes **yhat.model** as input and that enables us to have a dataframe that comprises the probability of model class prediction and the label of the test set.

`logloss()`: The metric for evaluating performance of each classifier that was used in the heart disease data. This function as two input namely **actual** which is the true class label from the test set and **predicted** which is the probability of the classifier's class prediction.


```{r R_Function, message= F, comment= F, warning= F}

# Probability of class prediction function to get the probability of the model prediction for the class labels

prob.prediction <- function(yhat.model){
  as_tibble(yhat.model) %>% mutate(prob=if_else(absent > present, absent, present)) %>% add_column(ytest= as.numeric(if_else(ytest == "absent", 0, 1))) 
}


# Logloss metric for evaluating performance of classifier

logloss = function(actual, predicted, eps = 1e-15) {
  yhat = pmin(pmax(predicted, eps), 1-eps)
  logloss <-  - (mean(actual * log(yhat) + (1 - actual) * log(1 - yhat)))
  return(logloss)
}

```

```{r model building, message= F, comment= F, warning= F}

# Models to consider


# LDA model

lda.model<- train(y ~ ., data = xytrain, method = "lda", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary), metric = "ROC")

yhat.lda <- predict(lda.model, xtest, type = "prob")

lda_data <- prob.prediction(yhat.model = yhat.lda)

logloss_lda <- logloss(lda_data$ytest, lda_data$prob)

# GBM model

GBM.model <- train(y~., data=xytrain, method = "gbm", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary, seeds = vector(mode = "list", length = nrow(xytrain) + 1) %>% lapply(., function(x) 1:20)), metric = "ROC", tuneGrid = expand.grid(interaction.depth = 1:2, shrinkage = .1, n.trees = c(10, 50, 100), n.minobsinnode = 10),verbose = FALSE)

yhat.GBM <- predict(GBM.model, xtest, type = "prob")

GBM_data <- prob.prediction(yhat.model = yhat.GBM)

logloss_GBM <- logloss(GBM_data$ytest, GBM_data$prob)


# SVM model

svm.model <- train(y ~ ., data = xytrain, method = "svmLinear2", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE), tuneGrid = data.frame(cost = c(.25, .5, 1)))

yhat.svm <- predict(svm.model, xtest, type = "prob")

svm_data <- prob.prediction(yhat.model = yhat.svm)

logloss_svm <- logloss(svm_data$ytest, svm_data$prob)


# KNN model

knn.model <- train(y ~.,data=xytrain, method = "knn", trControl=trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary), metric = "ROC")

yhat.knn <- predict(knn.model, newdata = xtest, type="prob")

knn_data <- prob.prediction(yhat.model = yhat.knn)

logloss_knn <- logloss(knn_data$ytest, knn_data$prob)


# Ctree

ctree.model <- train(y ~ ., data = xytrain, method = "ctree",  trControl = trainControl(method = "cv", number = 10, returnResamp = "all"))

yhat.ctree <- predict(ctree.model, newdata = xtest, type="prob")


ctree_data <- prob.prediction(yhat.model = yhat.ctree)

logloss_ctree <- logloss(ctree_data$ytest, ctree_data$prob)


# CART

cart.model <- train(y ~ ., data = xytrain, method = "rpart",  trControl = trainControl(method = "cv", number = 10, returnResamp = "all"))

yhat.cart <- predict(cart.model, newdata = xtest, type="prob")


cart_data <- prob.prediction(yhat.model = yhat.cart)

logloss_cart <- logloss(cart_data$ytest, cart_data$prob)


# cforest

cforest.model <- train(y ~ ., data = xytrain, method = "cforest", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary, seeds = vector(mode = "list", length = nrow(xytrain) + 1) %>% lapply(., function(x) 1:20)), metric = "ROC", controls = party::cforest_unbiased(ntree = 20))


yhat.cforest <- predict(cforest.model, newdata = xtest, type="prob")

cforest_data <- prob.prediction(yhat.model = yhat.cforest)

logloss_cforest <- logloss(cforest_data$ytest, cforest_data$prob)


# gausspr model

gausspr.model <- gausspr(y~., data=xytrain)
yhat.gausspr <- predict(gausspr.model, xtest, type='prob')

gausspr_data <- prob.prediction(yhat.model = yhat.gausspr)

logloss_gausspr <- logloss(gausspr_data$ytest, gausspr_data$prob)

# rForest

rforest.model <- train(y ~ ., data = xytrain, method = "rf", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE,summaryFunction = twoClassSummary, seeds = vector(mode = "list", length = nrow(xytrain) + 1) %>% lapply(., function(x) 1:20)),metric = "ROC", ntree = 20,importance = TRUE)

yhat.rforest <- predict( rforest.model, xtest, type='prob')

rforest_data <- prob.prediction(yhat.model = yhat.rforest)

logloss_rforest <- logloss(rforest_data$ytest, rforest_data$prob)


# Adaboost

adaboost.model <- train(y ~ ., data = xytrain, method = "adaboost", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary, seeds =vector(mode = "list", length = nrow(xytrain) + 1) %>% lapply(., function(x) 1:20)),metric = "ROC")


yhat.adaboost <- predict( adaboost.model, xtest, type='prob')

adaboost_data <- prob.prediction(yhat.model = yhat.adaboost)

logloss_adaboost <- logloss(adaboost_data$ytest, adaboost_data$prob)

# Nnet

nnet.model <- train(y ~ ., data = xytrain, method = "nnet", trControl = trainControl(method = "cv", number = 10, returnResamp = "all"),
trace = FALSE)

yhat.nnet <- predict(nnet.model, xtest, type="prob")

nnet_data <- prob.prediction(yhat.model = yhat.nnet)

logloss_nnet <- logloss(nnet_data$ytest, nnet_data$prob)


# LogitBoost

logit.model <- train(y ~ ., data = xytrain, method = "LogitBoost", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary), metric = "ROC")

yhat.logit <- predict(logit.model, xtest, type="prob")

logit_data <- prob.prediction(yhat.model = yhat.logit)

logloss_logit <- logloss(logit_data$ytest, logit_data$prob)


# NaiveBayes

naiveBayes.model <- train( y ~ ., data = xytrain, method = "naive_bayes", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary), metric = "ROC")

yhat.naiveBayes <- predict(naiveBayes.model, xtest, type='prob')

naiveBayes_data <- prob.prediction(yhat.model = yhat.naiveBayes)

logloss_naiveBayes <- logloss(naiveBayes_data$ytest, naiveBayes_data$prob)

# MARS model

mars.model <- train(y~., data=xytrain, method='earth', trControl=trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE),   tuneGrid = data.frame(degree = 1, nprune = (2:4)*2))

yhat.mars  <- predict(mars.model, xtest, type='prob')

mars_data <- prob.prediction(yhat.model = yhat.mars)

logloss_mars <- logloss(mars_data$ytest, mars_data$prob)

# glmnet model

glmnet.model <- train(y~., data=xytrain, method='glmnet',   trControl =trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary),metric = "ROC", tuneGrid = expand.grid(.alpha = seq(.05, 1, length = 15), .lambda = c((1:5)/10)))

yhat.glmnet  <- predict(glmnet.model, xtest, type='prob')

glmnet_data <- prob.prediction(yhat.model = yhat.glmnet)

logloss_glmnet <- logloss(glmnet_data$ytest, glmnet_data$prob)


# xgbTree

xgbtree.model <- train(y ~ ., data = xytrain, method = "xgbTree", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary), metric = "ROC", tuneGrid =  expand.grid(nrounds = c(1, 10), max_depth = c(1, 4), eta = c(.1, .4), gamma = 0, colsample_bytree = .7, min_child_weight = 1,subsample = c(.8, 1)))

yhat.xgbtree <- predict(xgbtree.model, xtest, type='prob')

xgbtree_data <- prob.prediction(yhat.model = yhat.xgbtree)

logloss_xgbtree <- logloss(xgbtree_data$ytest, xgbtree_data$prob)


j48.model <- train(y ~ ., data = xytrain, method = "J48", trControl = trainControl(method = "cv", number = 10, returnResamp = "all", classProbs = TRUE, summaryFunction = twoClassSummary), metric = "ROC")

yhat.j48 <- predict(j48.model, xtest, type='prob')

j48_data <- prob.prediction(yhat.model = yhat.j48)

logloss_j48 <- logloss(xgbtree_data$ytest, j48_data$prob)

```

```{r loglos_metric, message=F, comment=F, warning=F}

# Models logloss comparision to check for the best model

evaluation <- tibble(lda= logloss_lda, GBM= logloss_GBM, SVM =logloss_svm, KNN = logloss_knn, ctree = logloss_ctree, CART = logloss_cart, cforest = logloss_cforest, `gauss process` = logloss_gausspr, `Random forest` = logloss_rforest, Adaboost = logloss_adaboost, `Neural network` = logloss_nnet, "Logistic" = logloss_logit, Naivebayes = logloss_naiveBayes, MARS = logloss_mars, glmnet = logloss_glmnet, XGBtree =logloss_xgbtree, j48=logloss_j48)

logloss= t(evaluation)

Model = rownames(logloss)

comparision_table= as_tibble(logloss) %>% add_column(Model) %>% rename(logloss="V1") %>%  arrange(logloss) %>% add_column(SN= 1:length(Model),.after = 0) %>% dplyr::select(SN, Model, logloss)

```

> Evaluation table

```{r evaluation_table, eval= T, include= T, echo= F}

kable(comparision_table,align = c('c', 'c', 'c'), caption = "Models performance with logloss metric") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

```{r minlogloss}
# Model with minimin logloss

minlogloss <- comparision_table %>% filter(logloss == min(logloss))

kable(minlogloss, caption = 'Optimal model', align = c('c', 'c')) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

This shows that **`r minlogloss[,2]`** is the optimal model.


# Generalizing optimal model on validation dataset

We therefore, use the optimal model **`r minlogloss[,2]`** for predicting the class label on `validation` data.

```{r validation dataset, message=FALSE, warning=TRUE, comment=FALSE, cache=TRUE,include=TRUE}

# Validation dataset

validation_data <- read_csv("Data from DRIVENDATA/test_values.csv")

# Data wrangling and preprocessing of validation data

validation_data <- validation_data[-1] # patient_id column dropped

factor_variable_position <- c(1, 2, 4, 6, 7, 10, 13)

validation_data<- validation_data %>% mutate_at(vars(factor_variable_position),as_factor)


# Applying preprocessing to the validation dataset

validation_data <-  predict(preProcess_scale_model,newdata = validation_data)

validation_dummy <-  predict(dummies, newdata = validation_data)

# Convert to dataframe

xvalidation <- as_tibble(validation_dummy)

# Remove low variance columns on validation set

xvalidation <- xvalidation %>% dplyr::select(-c(resting_ekg_results.1,
thal.fixed_defect))

# Predicting the class label

validation_label <- predict(adaboost.model, xvalidation)

# Convert to data frame

validation_label <- tibble(heart_disease_present=validation_label)

validation_table <- validation_label %>% count(heart_disease_present) %>% mutate(Percent = round(n/sum(n)*100, 2))

```

## Class prediction label

```{r Predicted_labels, eval=T, include=T, echo=F}
# Counting the number of each predicted class

kable(validation_table, caption = 'Summary of predicted labels on validation data', align = c('l','c')) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width = "900px", height = "10px")
```


```{r submit_format}

# Validation label prediction

submission_format <- read_csv("Data from DRIVENDATA/submission_format.csv")

submission_format <- submission_format %>% mutate(heart_disease_present=validation_label$heart_disease_present) 
```


```{r Validation class labels, eval=T, include=T, echo=F}
# Counting the number of each predicted class

kable(submission_format, caption = 'Validation data class prediction using optimal model (Adaboost classifier)', align = c('l','c')) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), fixed_thead = T) %>% scroll_box(width = "900px", height = "400px")


```

# Conclusion

This study considered $17$ different catalogs of machine learning models which are carefully selected from the set of parametric and/or non-parametric models to choose the best optimal model with the least **logloss**. `caret` package was used to tune our different models and the optimal model **`r minlogloss[,2]`** predicted (`r validation_table[1,2]`, `r paste0( validation_table[1,3], "%")`) has `r validation_table[1,1]` while (`r validation_table[2,2]`, `r paste0( validation_table[2,3], "%")`) were predicted to have `r validation_table[2,1]` of heart disease on the validation dataset.

---

If you like this writeup, you can also follow me on [Twitter](https://www.twitter.com/gbganalyst){target="_blank"} and [Linkedin](https://www.linkedin.com/in/ezekiel-ogundepo/){target="_blank"} for more update in `R` and `Python` for datascience.
